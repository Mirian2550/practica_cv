{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 9\n",
    "\n",
    "Tomar fotografías donde coexistan varios objetos en posiciones solapadas y no, en contextos de diferente complejidad. Luego, aplicar los algoritmos de segmentación propuestos y verificar los resultados de cada uno. Comentar qué diferencias observa. "
   ],
   "id": "7ba56cb2b23c5b06"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T05:11:34.740629Z",
     "start_time": "2024-04-25T05:11:30.524314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install timm\n",
    "!clear\n",
    "!pip install transformers\n"
   ],
   "id": "a9e776e1a821a29f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (0.9.16)\r\n",
      "Requirement already satisfied: torch in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from timm) (2.2.2)\r\n",
      "Requirement already satisfied: torchvision in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from timm) (0.17.2)\r\n",
      "Requirement already satisfied: pyyaml in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from timm) (6.0.1)\r\n",
      "Requirement already satisfied: huggingface_hub in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from timm) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from timm) (0.4.3)\r\n",
      "Requirement already satisfied: filelock in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (24.0)\r\n",
      "Requirement already satisfied: requests in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface_hub->timm) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from torch->timm) (1.12)\r\n",
      "Requirement already satisfied: networkx in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from torch->timm) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from torch->timm) (3.1.3)\r\n",
      "Requirement already satisfied: numpy in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from torchvision->timm) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.5)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "\u001B[H\u001B[2JRequirement already satisfied: transformers in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (4.40.1)\r\n",
      "Requirement already satisfied: filelock in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (3.13.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (24.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (2024.4.16)\r\n",
      "Requirement already satisfied: requests in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from transformers) (4.66.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miri/practica_cv/venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.0\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T05:11:34.749388Z",
     "start_time": "2024-04-25T05:11:34.744746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "import requests\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from transformers import DetrFeatureExtractor, DetrForSegmentation\n",
    "from transformers.models.detr.feature_extraction_detr import rgb_to_id"
   ],
   "id": "b750bdf878344dce",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T05:12:12.146312Z",
     "start_time": "2024-04-25T05:11:34.751439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "paths = ['../fotos/todos.jpeg', '/fotos/leche.jpg', '/fotos/jugo.jpg']\n",
    "segmented_paths = []\n",
    "\n",
    "for path in paths:\n",
    "    # Cargar la imagen\n",
    "    image = Image.open(path)\n",
    "\n",
    "    # Cargar el modelo y el extractor de características\n",
    "    feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
    "    model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\")\n",
    "\n",
    "    # Preparar la imagen para el modelo\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Paso forward\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Postprocesamiento\n",
    "    processed_sizes = torch.as_tensor(inputs[\"pixel_values\"].shape[-2:]).unsqueeze(0)\n",
    "    result = feature_extractor.post_process_panoptic(outputs, processed_sizes, threshold=0.85)[0]\n",
    "\n",
    "    # Convertir la segmentación a numpy\n",
    "    panoptic_seg = Image.open(io.BytesIO(result[\"png_string\"]))\n",
    "    panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n",
    "    panoptic_seg_id = rgb_to_id(panoptic_seg)\n",
    "\n",
    "    # Preparar la paleta de colores\n",
    "    palette = itertools.cycle(sns.color_palette())\n",
    "\n",
    "    # Crear la imagen segmentada\n",
    "    segmented_image = Image.fromarray(np.zeros_like(panoptic_seg, dtype=np.uint8))\n",
    "    draw = ImageDraw.Draw(segmented_image)\n",
    "\n",
    "    # Añadimos un mapeo manual de los IDs de las categorías a los nombres, basado en las clases comunes de COCO\n",
    "    COCO_LABELS = {\n",
    "        1: 'persona', 2: 'bicicleta', 3: 'coche', 4: 'motocicleta', 5: 'avión',\n",
    "        6: 'autobús', 7: 'tren', 8: 'camión', 9: 'barco', 10: 'semáforo',\n",
    "        11: 'hidrante', 13: 'señal de stop', 14: 'parquímetro', 15: 'banco', 16: 'pájaro',\n",
    "        17: 'gato', 18: 'perro', 19: 'caballo', 20: 'oveja', 21: 'vaca',\n",
    "        22: 'elefante', 23: 'oso', 24: 'cebra', 25: 'jirafa', 27: 'mochila',\n",
    "        28: 'paraguas', 31: 'bolso de mano', 32: 'corbata', 33: 'maleta', 34: 'frisbee',\n",
    "        36: 'tabla de snowboard', 37: 'pelota deportiva', 38: 'cometa', 39: 'bate de béisbol',\n",
    "        40: 'guante de béisbol', 41: 'patineta', 42: 'tabla de surf', 43: 'raqueta de tenis',\n",
    "        44: 'botella', 46: 'plato de vino', 47: 'taza', 48: 'tenedor', 49: 'cuchillo',\n",
    "        50: 'cuchara', 51: 'tazón', 52: 'banana', 53: 'manzana', 54: 'sándwich',\n",
    "        55: 'naranja', 56: 'brócoli', 57: 'zanahoria', 58: 'perrito caliente', 59: 'pizza',\n",
    "        60: 'donut', 61: 'pastel', 62: 'silla', 63: 'sofá', 64: 'maceta', 65: 'cama',\n",
    "        67: 'mesa de comedor', 70: 'inodoro', 72: 'TV', 73: 'computadora portátil', 74: 'ratón',\n",
    "        75: 'control remoto', 76: 'teclado', 77: 'teléfono celular', 78: 'microondas',\n",
    "        79: 'horno', 80: 'tostadora', 81: 'fregadero', 82: 'refrigerador', 84: 'libro',\n",
    "        85: 'reloj', 86: 'florero', 87: 'tijeras', 88: 'oso de peluche', 89: 'secador de pelo',\n",
    "        90: 'cepillo de dientes',\n",
    "    }\n",
    "\n",
    "    # Ajusta el bucle para dibujar segmentos y etiquetas correctamente\n",
    "    for segment_info in result[\"segments_info\"]:\n",
    "        class_id = segment_info[\"category_id\"]\n",
    "        class_name = COCO_LABELS.get(class_id, 'Desconocido')  # 'Desconocido' si el ID no está en el diccionario\n",
    "        id = segment_info[\"id\"]\n",
    "\n",
    "        # Generar la máscara para este segmento específico\n",
    "        mask = panoptic_seg_id == id\n",
    "        color = np.array(next(palette)) * 255  # Convertir el color a un array de numpy adecuado\n",
    "\n",
    "        # Convertir la máscara a una imagen de PIL para usarla como máscara en 'paste'\n",
    "        mask_image = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "\n",
    "        # Crear una imagen del color del segmento que tenga las dimensiones correctas\n",
    "        color_image = Image.new(\"RGB\", segmented_image.size, color=tuple(color.astype(int)))\n",
    "\n",
    "        # Pegar usando la máscara para aplicar solo este segmento\n",
    "        segmented_image.paste(color_image, (0,0), mask=mask_image)\n",
    "\n",
    "        # Dibujar el nombre de la clase en la posición inicial del segmento\n",
    "        draw = ImageDraw.Draw(segmented_image)\n",
    "        where = np.where(mask)\n",
    "        if where[0].size > 0 and where[1].size > 0:\n",
    "            x, y = np.min(where[1]), np.min(where[0])\n",
    "            draw.text((x, y), class_name, fill='white')\n",
    "\n",
    "    segmented_paths.append(segmented_image)"
   ],
   "id": "dd5bbefd52f96cf7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ae3e4c3df7247b0a9ed81ab6cbe5098"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miri/practica_cv/venv/lib/python3.10/site-packages/transformers/models/detr/feature_extraction_detr.py:38: FutureWarning: The class DetrFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use DetrImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dea1431fc00e45a891f2d04494b37801"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/172M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f9fbb3bd786f4dc1a9491c4631299b66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/102M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48eb72b2f1c240a9af4505639c74fdd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50-panoptic were not used when initializing DetrForSegmentation: ['detr.model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'detr.model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "`post_process_panoptic is deprecated and will be removed in v5 of Transformers, please use `post_process_panoptic_segmentation`.\n",
      "/Users/miri/practica_cv/venv/lib/python3.10/site-packages/transformers/models/detr/image_processing_detr.py:1652: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  np_seg_img = torch.ByteTensor(torch.ByteStorage.from_buffer(seg_img.tobytes()))\n",
      "/Users/miri/practica_cv/venv/lib/python3.10/site-packages/transformers/models/detr/feature_extraction_detr.py:28: FutureWarning: rgb_to_id has moved and will not be importable from this module from v5. Please import from transformers.image_transforms instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/fotos/leche.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 6\u001B[0m\n\u001B[1;32m      2\u001B[0m segmented_paths \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m path \u001B[38;5;129;01min\u001B[39;00m paths:\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m# Cargar la imagen\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;66;03m# Cargar el modelo y el extractor de características\u001B[39;00m\n\u001B[1;32m      9\u001B[0m     feature_extractor \u001B[38;5;241m=\u001B[39m DetrFeatureExtractor\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfacebook/detr-resnet-50-panoptic\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/practica_cv/venv/lib/python3.10/site-packages/PIL/Image.py:3277\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   3274\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[1;32m   3276\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[0;32m-> 3277\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3278\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   3280\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/fotos/leche.jpg'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b0837918c0a8ac0e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
