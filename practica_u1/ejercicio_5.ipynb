{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "82b6f90ff1d52094"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "Genere un video en un patio o en un hall de edificio donde en un principio se vea vacío y luego aparezca una persona. Mediante los métodos de motion detection (sin usar deep learning) logre una detección de la persona cuando entra al cuadro suponiendo la utilidad para una cámara de seguridad. \n",
    "Luego sobre el mismo video aplique los algoritmos de flujo denso y disperso que se mostraron en clase. \n",
    "Escriba una reflexión sobre los resultados en el formato md dentro del Jupyter Notebook."
   ],
   "id": "4b8424aea22ed0c0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-25T04:43:05.034950Z",
     "start_time": "2024-04-25T04:43:04.849893Z"
    }
   },
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Función actualizada para realizar diferencia de fotogramas con normalización:\n",
    "def process_frame_difference(new_image, prev_image, **kwargs):\n",
    "    # Convertir las imágenes a escala de grises\n",
    "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_RGB2GRAY)\n",
    "    prev_gray = cv2.cvtColor(prev_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Calcular la diferencia absoluta entre los fotogramas actual y anterior\n",
    "    frame_diff = cv2.absdiff(new_gray, prev_gray)\n",
    "\n",
    "    # Normalizar la imagen de diferencia\n",
    "    norm_diff = cv2.normalize(frame_diff, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Umbralizar la imagen para resaltar las diferencias\n",
    "    _, thresh = cv2.threshold(norm_diff, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Convertir la imagen umbralizada a color para mantener la consistencia con el video original\n",
    "    thresh_color = cv2.cvtColor(thresh, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    return thresh_color\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T04:43:13.421801Z",
     "start_time": "2024-04-25T04:43:05.037999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_capture = cv2.VideoCapture('../fotos/video2.mp4')\n",
    "\n",
    "# Lee el primer fotograma\n",
    "prev_frame = None\n",
    "ret, prev_frame = video_capture.read()\n",
    "\n",
    "# Itera sobre los fotogramas restantes\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Procesa la diferencia entre fotogramas\n",
    "    processed_frame = process_frame_difference(frame, prev_frame)\n",
    "\n",
    "    # Muestra el fotograma procesado\n",
    "    cv2.imshow('Processed Frame', processed_frame)\n",
    "\n",
    "    # Sale del bucle si se presiona la tecla 'q'\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Actualiza el fotograma anterior\n",
    "    prev_frame = frame.copy()\n",
    "\n",
    "# Libera los recursos y cierra las ventanas\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "4cfbd8c5c7252c90",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T04:43:13.429451Z",
     "start_time": "2024-04-25T04:43:13.423056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Función para procesar el flujo óptico denso\n",
    "def process_dense_optical_flow(new_image, prev_image):\n",
    "    # Convierte la nueva imagen a escala de grises\n",
    "    gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if not hasattr(process_dense_optical_flow, \"init_done\"):\n",
    "        process_dense_optical_flow.prev_gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "        process_dense_optical_flow.mask = np.zeros_like(new_image)\n",
    "        process_dense_optical_flow.mask[..., 1] = 255\n",
    "        process_dense_optical_flow.init_done = True\n",
    "\n",
    "    if process_dense_optical_flow.init_done:\n",
    "        prev_gray = process_dense_optical_flow.prev_gray\n",
    "        mask = process_dense_optical_flow.mask\n",
    "\n",
    "    # Calcula el flujo óptico\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Computa magnitud y ángulo de los vectores 2D\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Establece el tono de la imagen según la dirección del flujo óptico\n",
    "    mask[..., 0] = angle * 180 / np.pi / 2\n",
    "    # Establece el valor de la imagen según la magnitud del flujo óptico\n",
    "    mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Convierte de HSV a RGB\n",
    "    rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "    # Actualiza la imagen previa a gris\n",
    "    process_dense_optical_flow.prev_grayprev_gray = gray.copy()\n",
    "    return rgb"
   ],
   "id": "3b1a14c9954283eb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T04:43:41.596240Z",
     "start_time": "2024-04-25T04:43:13.432969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_capture = cv2.VideoCapture('../fotos/video2.mp4')\n",
    "\n",
    "# Lee el primer fotograma\n",
    "prev_frame = None\n",
    "ret, prev_frame = video_capture.read()\n",
    "\n",
    "# Itera sobre los fotogramas restantes\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Procesa la diferencia entre fotogramas\n",
    "    processed_frame = process_dense_optical_flow(frame, prev_frame)\n",
    "\n",
    "    # Muestra el fotograma procesado\n",
    "    cv2.imshow('Processed Frame', processed_frame)\n",
    "\n",
    "    # Sale del bucle si se presiona la tecla 'q'\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Actualiza el fotograma anterior\n",
    "    prev_frame = frame.copy()\n",
    "\n",
    "# Libera los recursos y cierra las ventanas\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "7c447c899bf54ca9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T04:43:41.607684Z",
     "start_time": "2024-04-25T04:43:41.597906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_sparse_optical_flow(new_image, prev_image):\n",
    "    # Preparamos las imagenes de trabajo\n",
    "    new_gray = cv2.cvtColor(new_image, cv2.COLOR_BGR2GRAY)\n",
    "    prev_gray_image = cv2.cvtColor(prev_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Verificar si ya se han detectado las características de Shi-Tomasi\n",
    "    if not hasattr(process_sparse_optical_flow, \"shi_tomasi_done\"):\n",
    "        # Definir parámetros para la detección de esquinas de Shi-Tomasi\n",
    "        feature_params = dict(maxCorners=300, qualityLevel=0.2, minDistance=2, blockSize=7)\n",
    "        # Detectar puntos característicos en la imagen\n",
    "        process_sparse_optical_flow.prev_points = cv2.goodFeaturesToTrack(new_gray, mask=None, **feature_params)\n",
    "        # Crear una máscara para dibujar el flujo óptico\n",
    "        process_sparse_optical_flow.mask = np.zeros_like(new_image)\n",
    "        # Marcar que se ha completado la detección de Shi-Tomasi\n",
    "        process_sparse_optical_flow.shi_tomasi_done = True\n",
    "\n",
    "    # Continuar si se ha completado la detección de Shi-Tomasi\n",
    "    if process_sparse_optical_flow.shi_tomasi_done:\n",
    "        prev_points = process_sparse_optical_flow.prev_points\n",
    "        mask = process_sparse_optical_flow.mask\n",
    "\n",
    "    # Parámetros para el flujo óptico de Lucas-Kanade\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    # Calcular el flujo óptico de Lucas-Kanade\n",
    "    new_points, status, error = cv2.calcOpticalFlowPyrLK(prev_gray_image, new_gray, prev_points, None, **lk_params)\n",
    "    # Filtrar puntos buenos\n",
    "    good_old = prev_points[status == 1]\n",
    "    good_new = new_points[status == 1]\n",
    "    color = (0, 255, 0)  # Color para el dibujo\n",
    "    # Dibujar el movimiento (flujo óptico)\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.astype(int).ravel()\n",
    "        c, d = old.astype(int).ravel()\n",
    "        mask = cv2.line(mask, (a, b), (c, d), color, 2)\n",
    "        new_image = cv2.circle(new_image, (a, b), 3, color, -1)\n",
    "\n",
    "    # Combinar la imagen actual con las líneas de flujo óptico dibujadas\n",
    "    output = cv2.add(new_image, mask)\n",
    "    # Actualizar puntos para el siguiente cuadro\n",
    "    process_sparse_optical_flow.prev_points = good_new.reshape(-1, 1, 2)\n",
    "    return output"
   ],
   "id": "4548e9eb74ff6398",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T04:43:50.838221Z",
     "start_time": "2024-04-25T04:43:41.609501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_capture = cv2.VideoCapture('../fotos/video2.mp4')\n",
    "\n",
    "# Lee el primer fotograma\n",
    "prev_frame = None\n",
    "ret, prev_frame = video_capture.read()\n",
    "\n",
    "# Itera sobre los fotogramas restantes\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Procesa la diferencia entre fotogramas\n",
    "    processed_frame = process_sparse_optical_flow(frame, prev_frame)\n",
    "\n",
    "    # Muestra el fotograma procesado\n",
    "    cv2.imshow('Processed Frame', processed_frame)\n",
    "\n",
    "    # Sale del bucle si se presiona la tecla 'q'\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Actualiza el fotograma anterior\n",
    "    prev_frame = frame.copy()\n",
    "\n",
    "# Libera los recursos y cierra las ventanas \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "b9c17ebab0befa5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Reflexión:\n",
    "\n",
    "Los algoritmos aplicados al video grabado dentro de una casa revelaron resultados satisfactorios en cuanto a su utilidad como detectores de movimientos.\n",
    "\n",
    "El algoritmo de flujo óptico denso al comienzo pareciera que no detecta nada pero resultó eficaz al identificar claramente la presencia de una silueta en movimiento. Este método logro detectar cambios significativos y delinear la forma en movimiento.\n",
    "\n",
    "Por otro lado, el algoritmo de flujo disperso, realizó el seguimiento del movimiento de la persona, proporcionando líneas que delinean el trayecto seguido por la persona en movimiento.\n",
    "\n",
    "Es esencial tener en cuenta diversos factores, como la calidad de la cámara, su posición y las condiciones de iluminación, para garantizar un enfoque óptimo en la detección de movimientos. Estos aspectos tienen un impacto significativo en la precisión y fiabilidad de los resultados obtenidos, lo que asegura una detección de movimientos precisa y confiable.\n"
   ],
   "id": "ab4f309b7bb8b82f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
